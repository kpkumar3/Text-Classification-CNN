{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import date,datetime,timedelta\n",
    "import csv\n",
    "import re  # regular expressions (for playing with the text)\n",
    "#import string\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#Sentiment intensity analyzer from nltk\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import ldamodel\n",
    "from gensim.parsing.preprocessing import STOPWORDS # common english \"stop words\" -- a, the, etc.\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer #to tokenize glove dataset and documents\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense,Flatten,Embedding,Activation,Dropout,GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier #Keras wrapper for scikit learn (gridsearch)\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63977, saving model to best.weights.hdf5\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.63977\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.63977\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.63977\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.63977 to 0.77522, saving model to best.weights.hdf5\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77522\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77522\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77522\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77522\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77522\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77522\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.77522 to 0.86455, saving model to best.weights.hdf5\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.86455\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86455\n"
     ]
    }
   ],
   "source": [
    "def get_subjects(text):\n",
    "    if len(text) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        lst1 = text.split(\";\")\n",
    "        lst2 = []\n",
    "        for sbj in lst1:\n",
    "            try:\n",
    "                #lst2.append(sbj.split(\"(\"))\n",
    "                tmp_list = sbj.split(\"(\")\n",
    "                if int(tmp_list[1][:-2]) > 80:\n",
    "                    lst2.append(tmp_list[0])\n",
    "            except:\n",
    "                pass\n",
    "        if len(lst2) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return \",\".join(lst2)\n",
    "\n",
    "swords=stopwords.words('english')  #stopwords from nltk\n",
    "for i in STOPWORDS:   #STOPWORDS from gensim. Combining both to create a comprehensive list of stopwords.\n",
    "    swords.append(i)\n",
    "add = ['pm','jan','feb','mar','apr','may','jun','jul','aug','sep','dec','oct','nov','said','emailtoken','numbertoken','percenttoken', 'moneytoken', 'http', 'said', 'download_tabl', 'flow_ifram','countertoken', 'urltoken','uht']\n",
    "swords.extend(add)\n",
    "\n",
    "\n",
    "#Definitions of functions used in the program:\n",
    "def remove_nonascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def separate_hashtags(text):\n",
    "    return set(part[1:] for part in text.split() if part.startswith(\"#\"))\n",
    "        \n",
    "def replace_abbrevs(text):\n",
    "    return re.sub(r'([a-zA-Z])([\\'\\-\\.])(?=[a-zA-Z])', r'\\1', text)\n",
    "\n",
    "def replace_email(text):\n",
    "    return re.sub(r'[\\w\\.\\-]+@[\\w\\.\\-]+', '', text)\n",
    "\n",
    "def replace_urls(text):\n",
    "    ##'''https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python'''\n",
    "    return re.sub('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',r'',text)\n",
    "\n",
    "def replace_numbers(text):\n",
    "    price_exp = re.compile(r\"\\$(\\d*\\,){,}\\d+\\.?\\d*\")\n",
    "    pct_exp = re.compile(r'\\b(\\d*\\,){,}\\d+\\.?\\d*\\%')\n",
    "    counter_exp = re.compile(r\"(\\d*\\,){,}\\d+\\.?\\d*(st|nd|rd|th)s?\")\n",
    "    num_exp = re.compile(r\"\\b(\\d*\\,){,}\\d+\\.?\\d*\\b\")\n",
    "    text = re.sub(price_exp, '', text)\n",
    "    text = re.sub(pct_exp, '', text)\n",
    "    text = re.sub(counter_exp, '', text)\n",
    "    return re.sub(num_exp, '', text)\n",
    "\n",
    "def remove_punctuations(translator,text):  #Not used\n",
    "    return ' '.join(text.translate(translator).split()).lower()\n",
    "\n",
    "def read_corpus_basic(corp):  #Not used\n",
    "    for doc in corp:\n",
    "        yield [x for x in gensim.utils.simple_preprocess(doc, deacc=True)]\n",
    "\n",
    "#Calls all the text processing functions above and cleans the text.\n",
    "def clean(text): \n",
    "    Text = replace_email(text)\n",
    "    Text = replace_urls(Text)\n",
    "    Text = replace_abbrevs(Text)\n",
    "    Text = replace_numbers(Text)\n",
    "    Text = remove_nonascii(Text)\n",
    "    return Text\n",
    "\n",
    "#Removes stopwords, preprocesses to remove punctuations. Corp should be a list of documents\n",
    "def read_corpus_with_stemming_and_SW_removal(corp):\n",
    "    for doc in corp:\n",
    "        yield [lemmatizer.lemmatize(x) for x in gensim.utils.simple_preprocess(doc, deacc=True)\n",
    "                   if x.lower() not in swords]\n",
    "\n",
    "#Takes the df column(series of documents) and returns a list of cleaned texts. \n",
    "#The tokens(cleaned words in each text) contain bigrams, trigrams and are separated by space.\n",
    "def get_tokens(df):\n",
    "    print('pre-processing with reg exp')\n",
    "    # replace non-ascii characters, apostrophes, periods inside of words, urls, and numbers, using reg exp\n",
    "    #apply 'clean' function on the text calls the other functions and cleans the data. \n",
    "    articles = df.Text.apply(clean)\n",
    "    \n",
    "    print('reading tokens from text')\n",
    "    corp2 = list(read_corpus_with_stemming_and_SW_removal(articles))\n",
    "    #print(corp2)\n",
    "    \n",
    "    print('adding bigrams')\n",
    "    # identify bigrams in the text descriptions\n",
    "    bigrams2 = gensim.models.phrases.Phrases(threshold=50)\n",
    "    bigrams2.add_vocab(corp2)\n",
    "    bigram_phraser2 = gensim.models.phrases.Phraser(bigrams2)\n",
    "    \n",
    "    print('adding trigrams')\n",
    "    # we can apply the bigram phraser again to look for trigrams\n",
    "    trigrams2 = gensim.models.phrases.Phrases(threshold=80)\n",
    "    trigrams2.add_vocab(bigram_phraser2[corp2])\n",
    "    trigram_phraser2 = gensim.models.phrases.Phraser(trigrams2)\n",
    "    \n",
    "    print('Done!')\n",
    "    return [\" \".join(trigram_phraser2[bigram_phraser2[tokens]]) for tokens in corp2]\n",
    "\n",
    "#Builds a sequential model. Pass the required parameters when function is called and when the model is fit. Default values are below.\n",
    "def define_model(embedding_layer,num_filters=64,kernel_size=3,optimizer='adam', dropout = 0.3, hidden_neurons=200,retrain=0): #,**kwargs\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(hidden_neurons, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))   #dense 1 and sigmoid activation since its binary classification\n",
    "    if retrain==1: #Once in a week, the model gets retrained with all the data\n",
    "        #importing weights from saved model\n",
    "        print('Importing weights from saved model')\n",
    "        model.load_weights(\"best.weights.hdf5\") #comment this for the first run\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc']) # compile the model\n",
    "    return model\n",
    "\n",
    "#integer encode and pad documents the training data. Similar to creating bag of words.\n",
    "#docs should be a list of articles\n",
    "#tokenizer is the tokenizer object created and fitted during training the data.\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode the documents to replace the words with the corresponding interged from word index.\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    #Now the docs are encoded with numbers but they are of different lengths. \n",
    "    #specify the max length and pad all the docs at the end (post) with zeroes if the lengh of docs is shorter.\n",
    "    padded_docs = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded_docs\n",
    "\n",
    "#Trains/retrains the model with X and Y data. Retrains if retrain = 1. Default is retrain = 0.\n",
    "def retrain(X,Y,retrain=0):\n",
    "    max_length = 400\n",
    "    tokenizer = Tokenizer() \n",
    "    #X is List of news articles.Y are the corresponding labels.\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    #Get the vocab size using the word index. \n",
    "    vocab_size = len(tokenizer.word_index) + 1 #(total number of unique words)\n",
    "    \n",
    "    #Read the glove file and create word embeddings\n",
    "    embeddings_index = dict()\n",
    "    f = open('Lexis_Output/glove.6B.100d.txt',encoding=\"utf8\")  #100d -> 100 dimensions\n",
    "    for line in f:\n",
    "        values = line.split() #each word contains a word and its coefficients\n",
    "        word = values[0]   #first element is the word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') #100 coeffs are after word. \n",
    "        embeddings_index[word] = coefs  #creates a dict of word and its coeffs. \n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    #We now have the words and its coeffs. \n",
    "    #Using this index, we should create a matrix with weights for each word in training \n",
    "    MAX_SEQUENCE_LENGTH = max_length # 4000\n",
    "    MAX_NB_WORDS = vocab_size # 29672\n",
    "    EMBEDDING_DIM = 100  #100d\n",
    "    word_index = tokenizer.word_index  #t is the tokenizer object.\n",
    "    #print(len(word_index))\n",
    "    \n",
    "    #initiate zero weights can use np.random.random to initiate random weights\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM)) #matrix of 29672,100\n",
    "    for word, i in word_index.items(): #iterate through the words in word_index\n",
    "        embedding_vector = embeddings_index.get(word) #get the vector for the word from embeddings dict\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    #embeding layer is ready to be used as the input layer in our model.\n",
    "    embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=False)\n",
    "    \n",
    "    #encode input data using encode_docs function. Pass the tokenizer, max length and raw input data.\n",
    "    Xtrain = encode_docs(tokenizer, max_length, X)\n",
    "    #encode label data\n",
    "    Ytrain = lb.fit_transform(Y)  #irrelevant is 0 and relevant is 1\n",
    "    # define model\n",
    "    model = define_model(embedding_layer,retrain=retrain)\n",
    "    # fit network\n",
    "    checkpointer = ModelCheckpoint(filepath='best.weights.hdf5', monitor='val_acc', verbose=1, save_best_only=True,mode='max')\n",
    "    model.fit(Xtrain, Ytrain, validation_split=0.2, batch_size = 5, epochs=30, shuffle=True, callbacks=[checkpointer], verbose=0)    \n",
    "    return tokenizer,model    \n",
    "\n",
    "#For each news article in the file, encodes the document, classifies it as relevant/irrelevant and returns the label, squashed value\n",
    "def classify(tokenizer,nn_model,text):\n",
    "    max_length=400\n",
    "    test_post = []\n",
    "    test_post.append(text)\n",
    "    Xtest = encode_docs(tokenizer,max_length, test_post)\n",
    "    i = nn_model.predict(Xtest, verbose=0, steps=None)\n",
    "    #i looks like [[0.026]]\n",
    "    if i[0][0]>0.5:\n",
    "        return (i[0][0],\"Relevant\")\n",
    "    else:\n",
    "        return (i[0][0],\"Irrelevant\")\n",
    "\n",
    "#Input is the topics information obtained from topic modeling.\n",
    "#Reformats the topics data into topic number, words and its prob as frequency and returns the info.\n",
    "def get_topic_words(topics,num_topics):\n",
    "    topic,words,freq,=[],[],[]\n",
    "    for i in topics:\n",
    "        for j in i[1]:\n",
    "            words.append(j[0])\n",
    "            freq.append(j[1]*1000)\n",
    "    for i in range(num_topics):\n",
    "        topic.extend(['Topic'+str(i+1)]*len(topics[0][1]))\n",
    "    topic_info = pd.DataFrame()\n",
    "    topic_info['Topic'] = pd.Series(topic)\n",
    "    topic_info['Word'] = pd.Series(words)\n",
    "    topic_info['Frequency'] = pd.Series(freq)\n",
    "    return(topic_info)\n",
    "\n",
    "#topic modeling on the cleaned tokens. Input is a list of cleaned news articles with lemmatized and nonstop words. \n",
    "def topic_model(articles,num_topics,num_words,passes):\n",
    "    docs = []\n",
    "    for text in articles:\n",
    "        tokens=gensim.utils.simple_preprocess(text, deacc=True, min_len=3)\n",
    "        #non_stop_tokens = [w for w in tokens if w not in swords]\n",
    "        docs.append(tokens)\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in docs]\n",
    "    gmodel = gensim.models.ldamodel.LdaModel(corpus=corpus,num_topics=num_topics,id2word=dictionary,passes=100)\n",
    "    topics_data=gmodel.show_topics(num_topics=num_topics, num_words=num_words,formatted=False)\n",
    "    topics_words=get_topic_words(topics=topics_data,num_topics=num_topics)\n",
    "    return topics_words\n",
    "\n",
    "def write_data(file_name,data):\n",
    "    with open(file_name,\"a\") as f:\n",
    "        wr = csv.writer(f,delimiter=\",\")\n",
    "        wr.writerow(data)\n",
    "\n",
    "#Checks if a file is available. Returns a boolean value.\n",
    "def isFileAvailable(filename):\n",
    "    try:\n",
    "        #pd.read_csv('topics_words.csv')\n",
    "        with open(filename, 'r') as test:\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        #print('Warning: ',e)\n",
    "        print('Creating %s..' %filename)\n",
    "        return False\n",
    "\n",
    "#Can be used in future to read files from HDFS by submitting commands on command line.\n",
    "# import subprocess \n",
    "# def run_cmd(args_list):\n",
    "#         print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "#         proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         s_output, s_err = proc.communicate()\n",
    "#         s_return =  proc.returncode\n",
    "#         return s_return, s_output, s_err\n",
    "\n",
    "\n",
    "#df=pd.read_csv('MasterCopy.csv',index_col=None, header=0)\n",
    "#df['Tokens'] = get_tokens(df[['Text']])\n",
    "#df['Classification']=df['Label'].apply(lambda x: 'Relevant' if x.lower().strip() != 'irrelevant' else 'Irrelevant')\n",
    "#df[['Text','Tokens','Classification']].to_csv('MasterCopyBkp.csv',index_col=None)\n",
    "#df[['Text','Tokens','Classification']].to_csv('MasterCopy.csv',index=False)\n",
    "\n",
    "\n",
    "#Run this once to train the basic model using master data to create the tokenizer and model.\n",
    "df=pd.read_csv('MasterCopy.csv', index_col=None,header=0)\n",
    "X = list(df['Tokens'])\n",
    "Y = list(df['Classification'])\n",
    "tokenizer,nn_model=retrain(X,Y,retrain=0)\n",
    "\n",
    "\n",
    "#Sample texts to check the model\n",
    "#text1= 'this text has no relevant information see how gets classified. It might be disappointing if it cant classify the text appropriately but lets try to see if it can do any better than the basic machine learning models'\n",
    "#text2= 'FORT LAUDERDALE, Fla. — It was the ballots, not the machines.To have a chance at overcoming Gov. Rick Scotts 12,603-vote lead in their Senate race, incumbent Bill Nelson, D-Fla., desperately needed a manual statewide recount to show that tens of thousands of votes here in Democrat-heavy Broward County had been misread by scanners.That didnt happen. And Nelsons chances of holding his Senate seat went from very slim to virtually nonexistent. As Florida recount deadline looms, Nelsons chances of victory dwindle NOV. 17, 201802:01 I dont see a path, said Steve Schale, a veteran Democratic strategist based in Florida. \"Honestly, the path was never likely, but that doesnt diminish the need for a recount process, if for no other reason but to answer lingering questions — such as the undervotes in Broward, and provide certainty to all involved'\n",
    "#text3= 'a buckhead sky rise marketed as a pioneer in transit connectedness could be proving again that marta proximity pays off in the modern atlanta economy the story atlanta plaza in buckhead is being renamed salesforce tower atlanta in light of the global software company commitment to add new jobs in its regional headquarters over the next five years'\n",
    "#print(classify(tokenizer,nn_model,' '.join([lemmatizer.lemmatize(w) for w in gensim.utils.simple_preprocess(clean(text1),deacc=True)])))\n",
    "#print(classify(tokenizer,nn_model,' '.join([lemmatizer.lemmatize(w) for w in gensim.utils.simple_preprocess(clean(text2),deacc=True)])))\n",
    "#print(classify(tokenizer,nn_model,' '.join([lemmatizer.lemmatize(w) for w in gensim.utils.simple_preprocess(clean(text3),deacc=True)])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subjects(text):\n",
    "    if text == None or len(text) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        lst1 = text.split(\";\")\n",
    "        lst2 = []\n",
    "        for sbj in lst1:\n",
    "            try:\n",
    "                #lst2.append(sbj.split(\"(\"))\n",
    "                tmp_list = sbj.split(\"(\")\n",
    "                if int(tmp_list[1][:-2]) > 80:\n",
    "                    lst2.append(tmp_list[0])\n",
    "            except:\n",
    "                pass\n",
    "        if len(lst2) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return \",\".join(lst2)\n",
    "get_subjects(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 21:36:49.066914\n",
      "pre-processing with reg exp\n",
      "reading tokens from text\n",
      "adding bigrams\n",
      "adding trigrams\n",
      "Done!\n",
      "classifying..\n",
      "Filtering the relevant news articles to calculate sentiment..\n",
      "Creating Lexis_Output/Daily_News_Volume.csv..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:122: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:125: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Lexis_Output/topics_words.csv..\n",
      "Creating Lexis_Output/Lexis_Overall_Sentiment.csv..\n",
      "Data processed. The program runs again at 8AM tomorrow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:188: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:59:59.656983\n",
      "pre-processing with reg exp\n",
      "reading tokens from text\n",
      "adding bigrams\n",
      "adding trigrams\n",
      "Done!\n",
      "classifying..\n",
      "Filtering the relevant news articles to calculate sentiment..\n",
      "Data processed. The program runs again at 8AM tomorrow.\n",
      "2018-12-05 07:59:59.816020\n",
      "pre-processing with reg exp\n",
      "reading tokens from text\n",
      "adding bigrams\n",
      "adding trigrams\n",
      "Done!\n",
      "classifying..\n",
      "Filtering the relevant news articles to calculate sentiment..\n",
      "Data processed. The program runs again at 8AM tomorrow.\n",
      "2018-12-06 07:59:59.465265\n",
      "pre-processing with reg exp\n",
      "reading tokens from text\n",
      "adding bigrams\n",
      "adding trigrams\n",
      "Done!\n",
      "classifying..\n",
      "Filtering the relevant news articles to calculate sentiment..\n",
      "Data processed. The program runs again at 8AM tomorrow.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #This loop runs everyday. Program reads file from HDFS, processes it, creates required files and sleeps until next day.\n",
    "    while True:\n",
    "        #blank list to store all the data dictionaries from json file\n",
    "        all_articles = []\n",
    "        #temp dataframe to store formatted data after processing JSON file. \n",
    "        lexis_temp = pd.DataFrame()\n",
    "        #get current data and print.\n",
    "        curr_date=datetime.today()\n",
    "        date_var=str(curr_date.year)+'-'+str(curr_date.month).zfill(2)+'-'+str(curr_date.day).zfill(2)\n",
    "        print(curr_date)\n",
    "        \n",
    "        #Check for the day of the week and if retraining is required. Retrains every sunday.\n",
    "        if curr_date.weekday() == 6: #checking for Sunday\n",
    "            retrain_flag=1\n",
    "        else:\n",
    "            retrain_flag=0\n",
    "        #Get the filenames for HDFS folder.\n",
    "        client = InsecureClient('http://backend-0-3:50070', user='atl_sprint2018')\n",
    "        files = client.list('/data/atl_sprint_2018/lexis_archive/')\n",
    "        #Take the last vailable filename\n",
    "        fjson = files[-1]   \n",
    "        lastfile_date = datetime.strptime(fjson[6:14],'%Y%m%d')  #date of creation of last file. \n",
    "        delta = curr_date - lastfile_date  #calculate the difference between current date and the last file creation date.\n",
    "        missing_data = 0  #counter to see how many records in the json file are with empty data.\n",
    "        \n",
    "#another method to get data from hdfs. Can be used for GPU processing\n",
    "#       a=datetime.now()\n",
    "#       date=str(a.year)+str(a.month).zfill(2)+str(a.day).zfill(2)\n",
    "#       (ret, out, err)= run_cmd(['hadoop', 'fs', '-get', '/data/atl_sprint_2018/lexis_archive/lexis_%sT0000.json'\n",
    "#                           %(date), './SocialMediaSprint/'])\n",
    "#       if ret == 0:  #if return code is 0, file exists.\n",
    "#           with open('lexis_%sT0000.json' %(date), 'r') as file1:\n",
    "#       #Today’s file not found - If number of days between current date and last day’s file is > 0, \n",
    "#       #then today’s file is not found. Runs after 2 hours.\n",
    "        \n",
    "        if delta.days == 0:  #delta days = 0 ==> the last file was created today. Yay, we have data!\n",
    "            #read the file from hdfs\n",
    "            with client.read('/data/atl_sprint_2018/lexis_archive/' + fjson,encoding = 'utf-8',delimiter = '\\n') as file1:\n",
    "                for line in file1:  #each line is a json object (dictionary)\n",
    "                    try:\n",
    "                        news_article = json.loads(line)   \n",
    "                        if news_article['Text'] != 'None' and len(news_article['Text'].split(' '))>=100:  #consider the data which has more than 50 words\n",
    "                            all_articles.append(news_article)  #append individual news articles to the data list. \n",
    "                        else:\n",
    "                            missing_data+=1\n",
    "                    except:\n",
    "                        continue\n",
    "            #If there is data, processes the data and writes data into sentiment and topic files. \n",
    "            if len(all_articles) > 0:\n",
    "                lexis_temp['Text']=pd.Series(list(map(lambda news_article:\" \".join(news_article['Text'].split()),all_articles)))\n",
    "                lexis_temp['City'] = pd.Series(list(map(lambda news_article: news_article['City'], all_articles)))\n",
    "                lexis_temp['Comment_Count'] = pd.Series(list(map(lambda news_article: news_article['Comment_Count'], all_articles)))\n",
    "                lexis_temp['Country'] = pd.Series(list(map(lambda news_article: news_article['Country'], all_articles)))\n",
    "                lexis_temp['Data Source'] = pd.Series(list(map(lambda news_article: news_article['Data Source'], all_articles)))\n",
    "                lexis_temp['Description'] = pd.Series(list(map(lambda news_article: news_article['Description'], all_articles)))\n",
    "                lexis_temp['Favorite_Count'] = pd.Series(list(map(lambda news_article: news_article['Favorite_Count'], all_articles)))\n",
    "                lexis_temp['Headlines'] = pd.Series(list(map(lambda news_article: news_article['Headlines'], all_articles)))\n",
    "                lexis_temp['ID'] = pd.Series(list(map(lambda news_article: news_article['ID'], all_articles)))\n",
    "                lexis_temp['Language'] = pd.Series(list(map(lambda news_article: news_article['Language'], all_articles)))\n",
    "                lexis_temp['Location'] = pd.Series(list(map(lambda news_article: news_article['Location'], all_articles)))\n",
    "                lexis_temp['Original Source'] = pd.Series(list(map(lambda news_article: news_article['Original Source'], all_articles)))\n",
    "                lexis_temp['Screen_Name'] = pd.Series(list(map(lambda news_article: news_article['Screen_Name'], all_articles)))\n",
    "                lexis_temp['Share_Count'] = pd.Series(list(map(lambda news_article: news_article['Share_Count'], all_articles)))\n",
    "                lexis_temp['State'] = pd.Series(list(map(lambda news_article: news_article['State'], all_articles)))\n",
    "                lexis_temp['Time'] = pd.Series(list(map(lambda news_article: news_article['Time'], all_articles)))\n",
    "                lexis_temp['Time_Zone'] = pd.Series(list(map(lambda news_article: news_article['Time_Zone'], all_articles)))\n",
    "                lexis_temp['URL'] = pd.Series(list(map(lambda news_article: news_article['URL'], all_articles)))\n",
    "                lexis_temp['User_Name'] = pd.Series(list(map(lambda news_article: news_article['User_Name'], all_articles)))\n",
    "                lexis_temp['User_id'] = pd.Series(list(map(lambda news_article: news_article['User_id'], all_articles)))\n",
    "                lexis_temp['Tokens'] = pd.Series(get_tokens(lexis_temp[['Text']])) #get the cleaned tokens\n",
    "                lexis_temp['Subject']=pd.Series(lexis_temp['Description'].apply(lambda x: x.get('Subject')))\n",
    "                lexis_temp['Source'] = pd.Series(list(map(lambda news_article: news_article['Original Source'], all_articles)))\n",
    "                lexis_temp['Drop_Criteria']=pd.Series(lexis_temp['Text'].apply(lambda x: x.replace(\" \",\"\") if not None else x))\n",
    "                lexis_temp=lexis_temp.drop_duplicates(subset=['Drop_Criteria'],keep='first',inplace=False).reset_index(drop=True)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #Classifying the new articles.\n",
    "                print(\"classifying..\")\n",
    "                classification,confidence=[],[] \n",
    "                for t in lexis_temp['Tokens']:  #predict the output labels and the confidence of classification\n",
    "                    #note1: the classification confidence we receive is a value squashed by sigmoid which is \n",
    "                    #likely to be close to 1 or 0. How do we get the actual probability?\n",
    "                    conf,result=classify(tokenizer,nn_model,t) #tokenizer and nn_model get created and passed back after training/retraining.\n",
    "                    classification.append(result)\n",
    "                    confidence.append(conf)\n",
    "                lexis_temp['Classification'] = pd.Series(classification)  #Add data to the dataframe\n",
    "                lexis_temp['Confidence'] = pd.Series(confidence)                \n",
    "                \n",
    "                #Get only the rows with high confidence of classification to use them for for re-training.\n",
    "                #Note2: Related to note1. How do we get the articles classified with highest confidence? \n",
    "                df_newdata=lexis_temp[(lexis_temp['Confidence'] > 0.99) | (lexis_temp['Confidence'] <= 0.000001)].reset_index(drop=True)\n",
    "                #We accumulate the training samples for a week and retrain the model with old and new data together. \n",
    "                #we create a new file if NewTrainingData file does not exist. if it exists, we append the data to old file.\n",
    "                file_exists=False\n",
    "                file_exists = isFileAvailable('Lexis_Output/NewTrainingData.csv')\n",
    "                if file_exists:\n",
    "                    df_newdata[['Text','Tokens','Classification']].to_csv('Lexis_Output/NewTrainingData.csv',mode='a',index=False, header=False)\n",
    "                else:\n",
    "                    df_newdata[['Text','Tokens','Classification']].to_csv('Lexis_Output/NewTrainingData.csv',mode='a',index=False, header=True)\n",
    "                \n",
    "                               \n",
    "                #Take the relevant news_articles and calculate sentiment and topics                \n",
    "                print('Filtering the relevant news articles to calculate sentiment..')\n",
    "                df_relevant=lexis_temp[lexis_temp['Classification']=='Relevant'].reset_index(drop=True)  #Select only the relevant news articles to evaluate topics and sentiments\n",
    "                filedate = datetime.strftime((datetime.strptime(fjson[6:14],'%Y%m%d')-timedelta(days=1)),'%Y-%m-%d')\n",
    "                df_relevant['Date'] = pd.Series([filedate]*len(df_relevant))\n",
    "                \n",
    "                ########################Getting the daily volume in a .CSV file###############################\n",
    "                file_exists=False\n",
    "                file_exists=isFileAvailable('Lexis_Output/Daily_News_Volume.csv')\n",
    "                if file_exists:\n",
    "                    df_relevant.groupby(\"Date\").Text.count().reset_index(name=\"Volume\").to_csv(\"Lexis_Output/Daily_News_Volume.csv\",\n",
    "                                                                        sep=\",\",header=False,mode='a',index=False)\n",
    "                else:\n",
    "                    df_relevant.groupby(\"Date\").Text.count().reset_index(name=\"Volume\").to_csv(\"Lexis_Output/Daily_News_Volume.csv\",\n",
    "                                                                        sep=\",\",header=True,mode='a',index=False) \n",
    "                \n",
    "                ########################Getting the daily volume of Top 10 Sources in a .CSV file###############################\n",
    "                df_relevant.groupby(\"Source\").Text.count().reset_index(name=\"Volume\").sort_index(by=\"Volume\",\n",
    "                    ascending=False)[:10].to_csv(\"Lexis_Output/Lexis_Source_Volume.csv\",sep=\",\",header=True,mode='w',index=False)\n",
    "                \n",
    "                top_10_source= df_relevant.groupby(\"Source\").Text.count().reset_index(name=\"Volume\").sort_index(by=\"Volume\",\n",
    "                    ascending=False)[:10]\n",
    "                \n",
    "                ##################################Outputting the Data for Top 10 Sources###################################\n",
    "                \n",
    "                Output_Data=df_relevant[df_relevant['Source'].isin(list(top_10_source.Source))]\n",
    "                Output_Data['Todays_Date']=date_var\n",
    "                Output_Data[[\"Source\",\"Text\",\"Date\",\"Todays_Date\"]].to_csv('Lexis_Output/Top10_Source_Data.csv',sep=\",\",header=True,mode='w',index=False)\n",
    "                \n",
    "                ###########################################Subject Topics##################################################\n",
    "                \n",
    "                df_relevant['Subject_Topics'] = pd.Series(list(map(lambda news_article: get_subjects(news_article), df_relevant['Subject'])))\n",
    "                \n",
    "                topic_list=df_relevant['Subject_Topics'].tolist()\n",
    "                topic_list=[x.split(',') for x in topic_list if x is not None]\n",
    "                topic_list=[item.strip() for sublist in topic_list for item in sublist]\n",
    "                topic_count=Counter(topic_list)\n",
    "                topic_count=dict(topic_count)\n",
    "                topic_count=pd.Series(topic_count, name='Frequency')\n",
    "                topic_count.index.name='Topics'\n",
    "                topic_count.reset_index(name=\"Frequency\").to_csv('Lexis_Output/Topic_Subjects.csv',sep=\",\",header=True,mode='w',index=False)\n",
    "                \n",
    "                #prepare input for topic modeling.\n",
    "                articles=list(df_relevant['Tokens'])\n",
    "                num_topics = 5\n",
    "                num_words = 10\n",
    "                passes=100\n",
    "                topics_words=topic_model(articles,num_topics,num_words,passes)\n",
    "                topics_words['Date'] = pd.Series([filedate]*len(topics_words))\n",
    "                \n",
    "                \n",
    "                \n",
    "                #Write the reformatted topics data into a file. If the file is available, append the data, else write into a new file\n",
    "                file_exists=False\n",
    "                file_exists = isFileAvailable('Lexis_Output/topics_words.csv')\n",
    "                if file_exists:\n",
    "                    topics_words.to_csv('Lexis_Output/topics_words.csv', mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    topics_words.to_csv('Lexis_Output/topics_words.csv', mode='a', header=True, index=False)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #Calculate sentiment\n",
    "                daily_filename = 'Lexis_Output/Daily_File_%s.csv' % (filedate)\n",
    "                df_relevant['Sentiment'] = pd.Series([round(sid.polarity_scores(text)['compound'],1) for text in articles])\n",
    "                #df_relevant['Date'] = pd.Series([filedate]*len(df_relevant))\n",
    "                df_relevant[['Date', 'Text','Subject' ,'Sentiment', 'Classification']].to_csv(daily_filename,index=False)\n",
    "                df_relevant['Sentiment_Category']= df_relevant['Sentiment'].apply(lambda x: 'Negative' if x<0 else 'Postive' if x>0 else 'Neutral')\n",
    "                \n",
    "                \n",
    "                \n",
    "                #Getting Overall Sentiment Category\n",
    "                file_exists=False\n",
    "                file_exists = isFileAvailable('Lexis_Output/Lexis_Overall_Sentiment.csv')\n",
    "                if file_exists:\n",
    "                    df_relevant.groupby([\"Date\",\"Sentiment_Category\"]).Text.count().reset_index(name=\"Volume\").to_csv(\"Lexis_Output/Lexis_Overall_Sentiment.csv\",sep=\",\",header=False,mode='a',index=False)\n",
    "                else:\n",
    "                    df_relevant.groupby([\"Date\",\"Sentiment_Category\"]).Text.count().reset_index(name=\"Volume\").to_csv(\"Lexis_Output/Lexis_Overall_Sentiment.csv\",sep=\",\",header=True,mode='a',index=False)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #Getting Sentiment Category of Top 10 Sources\n",
    "                sent_source=df_relevant[df_relevant['Source'].isin(list(top_10_source.Source))]\n",
    "                sent_source.groupby([\"Date\",\"Source\",\"Sentiment_Category\"]).Text.count().reset_index(name=\"Volume\").sort_index(by=[\"Source\",\"Volume\"],\n",
    "                            ascending=False).to_csv(\"Lexis_Output/Lexis_Source_Sentiment.csv\",sep=\",\",header=True,mode='w',index=False)\n",
    "                \n",
    "                \n",
    "                if retrain_flag==1:\n",
    "                    #if weekday ==6 (sunday), retrain the model for next day's use.\n",
    "                    df_new=pd.read_csv('Lexis_Output/NewTrainingData.csv', index_col=None,header=0)  #training data accumulated over days\n",
    "                    df_master=pd.read_csv('Lexis_Output/MasterCopy.csv', index_col=None, header=0) #master copy with manually labelled data\n",
    "                    #append both and get new dataframe.\n",
    "                    df_c=df_new[['Tokens', 'Classification']].append(df_master[['Tokens', 'Classification']],ignore_index=True)\n",
    "                    X = list(df_c['Tokens'])\n",
    "                    Y = list(df_c['Classification'])\n",
    "                    print(\"Retraining with the new data..\")\n",
    "                    #return the new tokenizer and the retrained neural network model which will be used for prediction\n",
    "                    tokenizer,nn_model=retrain(X,Y,retrain=retrain_flag)\n",
    "                \n",
    "                #put the program to sleep to run at 8 AM the next day.\n",
    "                curr_date = datetime.now()\n",
    "                tomorrow = (curr_date + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "                delta=tomorrow-datetime.now()  #remaining time for today\n",
    "                print(\"Data processed. The program runs again at 8AM tomorrow.\")\n",
    "                time.sleep(delta.seconds +(3600*8))   #sleeps for remaining time today + 8 hours of next day.\n",
    "            #If there is no data in today's input file, code runs next day at 8AM.\n",
    "            else:\n",
    "                print(\"No data in file. The program runs again at 8AM tomorrow\")\n",
    "                curr_date = datetime.now()\n",
    "                tomorrow = (curr_date + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "                delta=tomorrow-datetime.now()\n",
    "                #break\n",
    "                #time.sleep(40)\n",
    "                time.sleep(delta.seconds +(3600*8))\n",
    "        #If today's file is not found, code runs after 2 hours. \n",
    "        else:\n",
    "            print(\"File not found, will try again after 2 hours\")\n",
    "            curr_date = datetime.now()\n",
    "            tomorrow = (curr_date + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            delta=tomorrow-datetime.now()\n",
    "            #break\n",
    "            #If the file is not found ever after 10 PM, the program gives up and sleeps till 8 AM next day\n",
    "            #In all the other cases, the program checks for file every 2 hours.\n",
    "            if delta.seconds/3600 < 2:\n",
    "                time.sleep(delta.seconds +(3600*8))\n",
    "            else: \n",
    "                #time.sleep(10)\n",
    "                time.sleep(7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
